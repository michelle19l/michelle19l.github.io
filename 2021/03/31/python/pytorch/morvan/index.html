<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>莫烦python pytorch学习 | Lxm's Blog</title><meta name="description" content="莫烦python pytorch学习"><meta name="keywords" content="python,机器学习,pytorch"><meta name="author" content="Lxm"><meta name="copyright" content="Lxm"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/smile.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="莫烦python pytorch学习"><meta name="twitter:description" content="莫烦python pytorch学习"><meta name="twitter:image" content="https://gitee.com/michelle19l/michelle19l/images/pytorch.jfif"><meta property="og:type" content="article"><meta property="og:title" content="莫烦python pytorch学习"><meta property="og:url" content="https://gitee.com/michelle19l/michelle19l/2021/03/31/python/pytorch/morvan/"><meta property="og:site_name" content="Lxm's Blog"><meta property="og:description" content="莫烦python pytorch学习"><meta property="og:image" content="https://gitee.com/michelle19l/michelle19l/images/pytorch.jfif"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://gitee.com/michelle19l/michelle19l/2021/03/31/python/pytorch/morvan/"><link rel="prev" title="pytorch中引入mnist数据集" href="https://gitee.com/michelle19l/michelle19l/2021/04/16/python/pytorch/mnist%E6%96%87%E4%BB%B6%E5%BC%95%E5%85%A5/"><link rel="next" title="机器学习/机器学习基础" href="https://gitee.com/michelle19l/michelle19l/2021/02/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: true,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">90</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">70</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">24</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/guide/"><i class="fa-fw fa fa-connectdevelop"></i><span> Guide</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> Book</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch和numpy"><span class="toc-number">1.</span> <span class="toc-text">pytorch和numpy</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#variable"><span class="toc-number">2.</span> <span class="toc-text">variable</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#激励函数"><span class="toc-number">3.</span> <span class="toc-text">激励函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#关系拟合"><span class="toc-number">4.</span> <span class="toc-text">关系拟合</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#分类"><span class="toc-number">5.</span> <span class="toc-text">分类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#快速搭建法"><span class="toc-number">6.</span> <span class="toc-text">快速搭建法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#保存提取"><span class="toc-number">7.</span> <span class="toc-text">保存提取</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#批训练"><span class="toc-number">8.</span> <span class="toc-text">批训练</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#优化器optimizer"><span class="toc-number">9.</span> <span class="toc-text">优化器optimizer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CNN"><span class="toc-number">10.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#数据处理"><span class="toc-number">10.1.</span> <span class="toc-text">数据处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN模型搭建"><span class="toc-number">10.2.</span> <span class="toc-text">CNN模型搭建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练"><span class="toc-number">10.3.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#输出"><span class="toc-number">10.4.</span> <span class="toc-text">输出</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RNN"><span class="toc-number">11.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#分类-1"><span class="toc-number">11.1.</span> <span class="toc-text">分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#回归"><span class="toc-number">11.2.</span> <span class="toc-text">回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LSTM"><span class="toc-number">12.</span> <span class="toc-text">LSTM</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/images/pytorch.jfif)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Lxm's Blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/guide/"><i class="fa-fw fa fa-connectdevelop"></i><span> Guide</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> Book</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">莫烦python pytorch学习</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2021-03-31 10:04:18"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2021-03-31</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2021-04-16 15:39:02"><i class="fa fa-history" aria-hidden="true"></i> Updated 2021-04-16</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/pytorch/">pytorch</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>Word count:</span><span class="word-count">4.6k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>Reading time: 23 min</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h1 id="pytorch和numpy"><a href="#pytorch和numpy" class="headerlink" title="pytorch和numpy"></a>pytorch和numpy</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy和torch的转换</span></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))<span class="comment"># 2行3列</span></span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nnumpy array:'</span>, np_data,          <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">    <span class="string">'\ntorch tensor:'</span>, torch_data,      <span class="comment">#  0  1  2 \n 3  4  5    [torch.LongTensor of size 2x3]</span></span><br><span class="line">    <span class="string">'\ntensor to array:'</span>, tensor2array, <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>如果是nparray，直接使用from_numpy(data)即可<br>如果是普通数组，使用torch.FloatTensor(data)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数学运算</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># abs 计算绝对值</span></span><br><span class="line">data=[<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">tensor=torch.FloatTensor(data)<span class="comment">#转换成32位浮点tensor</span></span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line"><span class="comment"># data=[-1,-2,1,2]</span></span><br><span class="line"><span class="comment"># tensor=torch.from_numpy(np.array(data))</span></span><br><span class="line"></span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nabs'</span>,</span><br><span class="line">    <span class="string">'\nnumpy: '</span>, np.abs(data),          <span class="comment"># [1 2 1 2]</span></span><br><span class="line">    <span class="string">'\ntorch: '</span>, torch.abs(tensor)      <span class="comment"># [1 2 1 2]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sin   三角函数 sin</span></span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nsin'</span>,</span><br><span class="line">    <span class="string">'\nnumpy: '</span>, np.sin(data),      <span class="comment"># [-0.84147098 -0.90929743  0.84147098  0.90929743]</span></span><br><span class="line">    <span class="string">'\ntorch: '</span>, torch.sin(tensor)  <span class="comment"># [-0.8415 -0.9093  0.8415  0.9093]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean  均值</span></span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nmean'</span>,</span><br><span class="line">    <span class="string">'\nnumpy: '</span>, np.mean(data),         <span class="comment"># 0.0</span></span><br><span class="line">    <span class="string">'\ntorch: '</span>, torch.mean(tensor)     <span class="comment"># 0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># matrix multiplication 矩阵点乘</span></span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 转换成32位浮点 tensor</span></span><br><span class="line"><span class="comment"># correct method</span></span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nmatrix multiplication (matmul)'</span>,</span><br><span class="line">    <span class="string">'\nnumpy: '</span>, np.matmul(data, data),     <span class="comment"># [[7, 10], [15, 22]]</span></span><br><span class="line">    <span class="string">'\ntorch: '</span>, torch.mm(tensor, tensor)   <span class="comment"># [[7, 10], [15, 22]]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!  下面是错误的方法 !!!!</span></span><br><span class="line">data = np.array(data)</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nmatrix multiplication (dot)'</span>,</span><br><span class="line">    <span class="string">'\nnumpy: '</span>, data.dot(data),        <span class="comment"># [[7, 10], [15, 22]] 在numpy 中可行</span></span><br><span class="line">    <span class="string">'\ntorch: '</span>, tensor.dot(tensor)     <span class="comment"># torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0 </span></span><br><span class="line">    <span class="comment"># 该写法在我的python里直接报错了</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h1 id="variable"><a href="#variable" class="headerlink" title="variable"></a>variable</h1><blockquote>
<p>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置. 里面的值会不停的变化. 就像一个裝鸡蛋的篮子, 鸡蛋数会不停变动. 那谁是里面的鸡蛋呢, 自然就是 Torch 的 Tensor 咯. 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)  <span class="comment"># 需要生成梯度信息</span></span><br><span class="line">print(tensor)</span><br><span class="line">print(variable)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>tensor([[1., 2.],<br>        [3., 4.]])<br>tensor([[1., 2.],<br>        [3., 4.]], requires_grad=True)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(tensor*tensor) <span class="comment"># x^2</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>tensor([[ 1.,  4.],<br>        [ 9., 16.]])</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t_out = torch.mean(tensor*tensor)       <span class="comment"># x^2</span></span><br><span class="line">v_out = torch.mean(variable*variable)   <span class="comment"># x^2</span></span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)    <span class="comment"># 7.5</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>tensor(7.5000)<br>tensor(7.5000, grad_fn=<MeanBackward0>)</MeanBackward0></p>
</blockquote>
<blockquote>
<p> <font color="green"><strong>Variable 计算时, 它在背景幕布后面一步步默默地搭建着一个庞大的系统, 叫做计算图, computational graph. 这个图是用来将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力.</strong></font></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v_out.backward()    <span class="comment"># 模拟 v_out 的误差反向传递</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面两步看不懂没关系, 只要知道 Variable 是计算图的一部分, 可以用来传递误差就好.</span></span><br><span class="line"><span class="comment"># v_out = 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤</span></span><br><span class="line"><span class="comment"># 针对于 v_out 的梯度就是, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span></span><br><span class="line"></span><br><span class="line">print(variable.grad)    <span class="comment"># 初始 Variable 的梯度</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> 0.5000  1.0000</span></span><br><span class="line"><span class="string"> 1.5000  2.0000</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(variable)     <span class="comment">#  Variable 形式</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">print(variable.data)    <span class="comment"># tensor 形式</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">print(variable.data.numpy())    <span class="comment"># numpy 形式</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[ 1.  2.]</span></span><br><span class="line"><span class="string"> [ 3.  4.]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h1 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h1><p>梯度爆炸和梯度消失</p>
<p>CNN-&gt; relu</p>
<p>rnn-&gt; relu or tanh</p>
<p>非线性函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment"># 激励函数都在这</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)  <span class="comment"># [-5,5] 200个均分</span></span><br><span class="line">x = Variable(x)</span><br><span class="line"></span><br><span class="line">x_np = x.data.numpy()  <span class="comment"># 画图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 激励函数</span></span><br><span class="line"></span><br><span class="line">y_relu = F.relu(x).data.numpy()</span><br><span class="line">y_sigmoid = F.sigmoid(x).data.numpy()</span><br><span class="line">y_tanh = F.tanh(x).data.numpy()</span><br><span class="line">y_softplus = F.softplus(x).data.numpy()</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.plot(x_np, y_relu, c=<span class="string">'red'</span>, label=<span class="string">'relu'</span>)</span><br><span class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.plot(x_np, y_sigmoid, c=<span class="string">'red'</span>, label=<span class="string">'sigmoid'</span>)</span><br><span class="line">plt.ylim((<span class="number">-0.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.plot(x_np, y_tanh, c=<span class="string">'red'</span>, label=<span class="string">'tanh'</span>)</span><br><span class="line">plt.ylim((<span class="number">-1.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.plot(x_np, y_softplus, c=<span class="string">'red'</span>, label=<span class="string">'softplus'</span>)</span><br><span class="line">plt.ylim((<span class="number">-0.2</span>, <span class="number">6</span>))</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<img src="/img/loading.gif" class="lazyload" data-src="/images/morvanpytorch/image-20210327185811769.png"  alt="image-20210327185811769" style="zoom: 80%;">

<h1 id="关系拟合"><a href="#关系拟合" class="headerlink" title="关系拟合"></a>关系拟合</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把一维数据编程二维</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line"><span class="comment"># 增加噪点</span></span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())  <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 输入个数和隐藏层神经元个数</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_features, n_hidden)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)  <span class="comment"># 输出层，如果只输出y，则output=1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.hidden(x))  <span class="comment"># 对隐藏层使用激励函数</span></span><br><span class="line">        x = self.predict(x)  <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(n_features=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD优化器</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)</span><br><span class="line">loss_func = torch.nn.MSELoss()  <span class="comment"># 均方差，用于回归问题</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):  <span class="comment"># 100步训练</span></span><br><span class="line">    prediction = net(x) <span class="comment"># 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)  <span class="comment"># 计算误差和真实的差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将所有参数的梯度降为0</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传递，计算参数更新值</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 以lr优化</span></span><br><span class="line">	<span class="comment"># 可以用于画图查看神经网络调整流程</span></span><br></pre></td></tr></table></figure>

<h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment"># 激励函数都在这</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line"><span class="comment"># 100行2列 1</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)  <span class="comment"># 数据的基本形态</span></span><br><span class="line"><span class="comment"># 正态分布(means每个输出元素的均值,std标准差,out=None可选的输出张量)</span></span><br><span class="line">x0 = torch.normal(<span class="number">2</span> * n_data, <span class="number">1</span>)  <span class="comment"># 类型0 x data (tensor), shape=(100, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一维行向量</span></span><br><span class="line">y0 = torch.zeros(<span class="number">200</span>)  <span class="comment"># 类型0 y data (tensor), shape=(100, )</span></span><br><span class="line">x1 = torch.normal(<span class="number">-2</span> * n_data, <span class="number">1</span>)  <span class="comment"># 类型1 x data (tensor), shape=(100, 1)</span></span><br><span class="line">y1 = torch.ones(<span class="number">200</span>)  <span class="comment"># 类型1 y data (tensor), shape=(100, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</span></span><br><span class="line"><span class="comment"># 有0，按行合并，没有按列</span></span><br><span class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.LongTensor)  <span class="comment"># FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)  <span class="comment"># LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># y=0,y=1两个颜色</span></span><br><span class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=y.data.numpy(), s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 输入个数和隐藏层神经元个数</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_features, n_hidden)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)  <span class="comment"># 输出层，如果只输出y，则output=1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.hidden(x))  <span class="comment"># 对隐藏层使用激励函数</span></span><br><span class="line">        x = self.predict(x)  <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 输入个数和隐藏层神经元个数</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_features, n_hidden)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)  <span class="comment"># 输出层，如果只输出y，则output=1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.hidden(x))  <span class="comment"># 对隐藏层使用激励函数</span></span><br><span class="line">        x = self.predict(x)  <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net(n_features=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>)  <span class="comment"># 分成2类，output=2</span></span><br><span class="line"><span class="comment"># 如果输出是[0,1]则分类为1</span></span><br><span class="line"><span class="comment"># 如果输出为[1,0]则分类为0</span></span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD优化器</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.05</span>)</span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()  <span class="comment"># 用于分类问题，softmax</span></span><br><span class="line"><span class="comment"># 例 输出[0.1,0.2,0.7] 概率，根据这种形式计算误差</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):  <span class="comment"># 100步训练</span></span><br><span class="line">    out = net(x)  <span class="comment"># 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(out, y)  <span class="comment"># 计算误差和真实的差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将所有参数的梯度降为0</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传递，计算参数更新值</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 以lr优化</span></span><br></pre></td></tr></table></figure>



<h1 id="快速搭建法"><a href="#快速搭建法" class="headerlink" title="快速搭建法"></a>快速搭建法</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net2=torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">2</span>,<span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU(), <span class="comment"># 类，搭建效果和class相同</span></span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Net(<br>  (hidden): Linear(in_features=2, out_features=10, bias=True)<br>  (predict): Linear(in_features=10, out_features=2, bias=True)<br>)<br>Sequential(<br>  (0): Linear(in_features=2, out_features=10, bias=True)<br>  (1): ReLU()<br>  (2): Linear(in_features=10, out_features=2, bias=True)<br>)</p>
</blockquote>
<h1 id="保存提取"><a href="#保存提取" class="headerlink" title="保存提取"></a>保存提取</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible,使每次的初始化固定</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())  <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 建网络</span></span><br><span class="line">    net1 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    optimizer = torch.optim.SGD(net1.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">    loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        prediction = net1(x)</span><br><span class="line">        loss = loss_func(prediction, y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    torch.save(net1, <span class="string">'net.pkl'</span>)  <span class="comment"># 保存整个网络</span></span><br><span class="line">	torch.save(net1.state_dict(), <span class="string">'net_params.pkl'</span>)   <span class="comment"># 只保存网络中的参数 (速度快, 占内存少)</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_net</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># restore entire net1 to net2</span></span><br><span class="line">    net2 = torch.load(<span class="string">'net.pkl'</span>)</span><br><span class="line">    prediction = net2(x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 新建 net3</span></span><br><span class="line">    net3 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将保存的参数复制到 net3</span></span><br><span class="line">    net3.load_state_dict(torch.load(<span class="string">'net_params.pkl'</span>))</span><br><span class="line">    prediction = net3(x)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 保存 net1 (1. 整个网络, 2. 只有参数)</span></span><br><span class="line">save()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取整个网络</span></span><br><span class="line">restore_net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取网络参数, 复制到新网络</span></span><br><span class="line">restore_params()</span><br></pre></td></tr></table></figure>

<h1 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h1><p>dataloader可以迭代地处理数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span>     <span class="comment"># 批训练的数据个数</span></span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>)       <span class="comment"># x data (torch tensor)</span></span><br><span class="line">y = torch.linspace(<span class="number">10</span>, <span class="number">1</span>, <span class="number">10</span>)       <span class="comment"># y data (torch tensor)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先转换成 torch 能识别的 Dataset</span></span><br><span class="line"><span class="comment"># 训练数据data_tensor,计算误差target_tensor</span></span><br><span class="line">torch_dataset = Data.TensorDataset(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 dataset 放入 DataLoader</span></span><br><span class="line">loader = Data.DataLoader(</span><br><span class="line">    dataset=torch_dataset,      <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size=BATCH_SIZE,      <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,               <span class="comment"># 要不要打乱数据 (打乱比较好)</span></span><br><span class="line">    num_workers=<span class="number">0</span>,              <span class="comment"># 多线程来读数据，0为默认值</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">3</span>):   <span class="comment"># 训练所有!整套!数据 3 次</span></span><br><span class="line">    <span class="comment"># 可以设置是否打乱数据 shuffle</span></span><br><span class="line">    <span class="comment"># enumerate 索引</span></span><br><span class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> enumerate(loader):  <span class="comment"># 每一步 loader 释放一小批数据用来学习</span></span><br><span class="line">        <span class="comment"># 假设这里就是你训练的地方...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打出来一些数据</span></span><br><span class="line">        print(<span class="string">'Epoch: '</span>, epoch, <span class="string">'| Step: '</span>, step, <span class="string">'| batch x: '</span>,</span><br><span class="line">              batch_x.numpy(), <span class="string">'| batch y: '</span>, batch_y.numpy())</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Epoch:  0 | Step:  0 | batch x:  [ 5.  7. 10.  3.  4.  2.  1.  8.] | batch y:  [ 6.  4.  1.  8.  7.  9. 10.  3.]<br>Epoch:  0 | Step:  1 | batch x:  [9. 6.] | batch y:  [2. 5.]<br>Epoch:  1 | Step:  0 | batch x:  [ 4.  6.  7. 10.  8.  5.  3.  2.] | batch y:  [7. 5. 4. 1. 3. 6. 8. 9.]<br>Epoch:  1 | Step:  1 | batch x:  [1. 9.] | batch y:  [10.  2.]<br>Epoch:  2 | Step:  0 | batch x:  [ 4.  2.  5.  6. 10.  3.  9.  1.] | batch y:  [ 7.  9.  6.  5.  1.  8.  2. 10.]<br>Epoch:  2 | Step:  1 | batch x:  [8. 7.] | batch y:  [3. 4.]</p>
</blockquote>
<h1 id="优化器optimizer"><a href="#优化器optimizer" class="headerlink" title="优化器optimizer"></a>优化器optimizer</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)  <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">EPOCH = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fake dataset</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1000</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.1</span> * torch.normal(torch.zeros(*x.size()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot dataset</span></span><br><span class="line">plt.scatter(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用上节内容提到的 data loader</span></span><br><span class="line">torch_dataset = Data.TensorDataset(x, y)</span><br><span class="line">loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认的 network 形式</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(<span class="number">1</span>, <span class="number">20</span>)  <span class="comment"># hidden layer</span></span><br><span class="line">        self.predict = torch.nn.Linear(<span class="number">20</span>, <span class="number">1</span>)  <span class="comment"># output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.hidden(x))  <span class="comment"># activation function for hidden layer</span></span><br><span class="line">        x = self.predict(x)  <span class="comment"># linear output</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个优化器创建一个 net</span></span><br><span class="line">net_SGD = Net()</span><br><span class="line">net_Momentum = Net()</span><br><span class="line">net_RMSprop = Net()</span><br><span class="line">net_Adam = Net()</span><br><span class="line">nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]</span><br><span class="line"></span><br><span class="line"><span class="comment"># different optimizers</span></span><br><span class="line">opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line">opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line">opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line">opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br><span class="line">optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</span><br><span class="line"></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line">losses_his = [[], [], [], []]  <span class="comment"># 记录 training 时不同神经网络的 loss</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):  <span class="comment"># EPOCH=12</span></span><br><span class="line">    print(<span class="string">'Epoch: '</span>, epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> enumerate(loader):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对每个优化器, 优化属于他的神经网络</span></span><br><span class="line">        <span class="keyword">for</span> net, opt, l_his <span class="keyword">in</span> zip(nets, optimizers, losses_his):  <span class="comment"># 后者都是list形式</span></span><br><span class="line">            output = net(b_x)  <span class="comment"># get output for every net</span></span><br><span class="line">            loss = loss_func(output, b_y)  <span class="comment"># compute loss for every net</span></span><br><span class="line">            opt.zero_grad()  <span class="comment"># clear gradients for next train</span></span><br><span class="line">            loss.backward()  <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">            opt.step()  <span class="comment"># apply gradients</span></span><br><span class="line">            l_his.append(loss.data.numpy())  <span class="comment"># loss recoder</span></span><br></pre></td></tr></table></figure>

<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>收集一小块的图像信息，进行总结</p>
<p>mnist</p>
<p>mnist数据集放在root下的<code>\MNIST\processed</code>文件夹下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision  <span class="comment"># 数据库模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span>  <span class="comment"># 训练整批数据多少次, 为了节约时间, 我们只训练一次</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">LR = <span class="number">0.001</span>  <span class="comment"># 学习率</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">False</span>  <span class="comment"># 如果你已经下载好了mnist数据就写上 False</span></span><br><span class="line"></span><br><span class="line">train_data_ = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">'./mnist'</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),  <span class="comment"># np array(pixel)修改为tensor （0，255）-&gt;（0，1）</span></span><br><span class="line">    download=DOWNLOAD_MNIST</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 可以打印出数字图片</span></span><br><span class="line">plt.imshow(train_data_.train_data[<span class="number">1</span>].numpy(), cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(train_data_)</span><br><span class="line">print(train_data_.train_data)</span><br></pre></td></tr></table></figure>



<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision  <span class="comment"># 数据库模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span>  <span class="comment"># 训练整批数据多少次, 为了节约时间, 我们只训练一次</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">LR = <span class="number">0.001</span>  <span class="comment"># 学习率</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">False</span>  <span class="comment"># 如果你已经下载好了mnist数据就写上 False</span></span><br><span class="line"></span><br><span class="line">train_data_ = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">'./mnist'</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),  <span class="comment"># np array(pixel)修改为tensor （0，255）-&gt;（0，1）</span></span><br><span class="line">    download=DOWNLOAD_MNIST</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data_ = torchvision.datasets.MNIST(root=<span class="string">'./mnist'</span>, train=<span class="literal">False</span>)</span><br><span class="line">train_loader = Data.DataLoader(dataset=train_data_, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为简化训练，只训练2000组</span></span><br><span class="line"><span class="comment"># 将整数转换成小数，再除以255</span></span><br><span class="line">test_x = torch.unsqueeze(test_data_.test_data, dim=<span class="number">1</span>).type(torch.FloatTensor)[:<span class="number">2000</span>] / <span class="number">255</span></span><br><span class="line">test_y = test_data_.test_labels[:<span class="number">2000</span>]</span><br></pre></td></tr></table></figure>

<h2 id="CNN模型搭建"><a href="#CNN模型搭建" class="headerlink" title="CNN模型搭建"></a>CNN模型搭建</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># CNN模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            <span class="comment"># (1,28,28)</span></span><br><span class="line">            nn.Conv2d(  <span class="comment"># 卷积层，filter（深度）☞在这一区域内提取出的特征个数</span></span><br><span class="line">                in_channels=<span class="number">1</span>,  <span class="comment"># 输入层高度，黑白1，彩色3</span></span><br><span class="line">                out_channels=<span class="number">16</span>,  <span class="comment"># filter个数，每块提取出16个特征</span></span><br><span class="line">                kernel_size=<span class="number">5</span>,  <span class="comment"># filter宽和高都是5pixel 5x5，每次扫描25个像素点</span></span><br><span class="line">                stride=<span class="number">1</span>,  <span class="comment"># 每步跳1个pixel，步长</span></span><br><span class="line">                padding=<span class="number">2</span>,  <span class="comment"># 边框，提取边缘像素点特征</span></span><br><span class="line">                <span class="comment"># 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1</span></span><br><span class="line">                <span class="comment"># output shape (16, 28, 28)</span></span><br><span class="line">            ),</span><br><span class="line">            nn.ReLU(),  <span class="comment"># 非线性激活层 # (16, 28, 28)</span></span><br><span class="line">            nn.MaxPool2d(  <span class="comment"># 池化层，筛选重要的特征</span></span><br><span class="line">                kernel_size=<span class="number">2</span>,  <span class="comment"># 长和宽为2的pooling，可以理解为提取2x2空间种的最大值，用于压缩长宽，但是高度不变</span></span><br><span class="line">            )  <span class="comment"># (16, 14, 14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(  <span class="comment"># (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>,  <span class="comment"># input, conv1 outchannel=16</span></span><br><span class="line">                      <span class="number">32</span>,</span><br><span class="line">                      <span class="number">5</span>,  <span class="comment"># kernel_size</span></span><br><span class="line">                      <span class="number">1</span>,</span><br><span class="line">                      <span class="number">2</span>, ),  <span class="comment"># (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),  <span class="comment"># (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)  <span class="comment"># 输入，输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>  <span class="comment"># x表示输入的training数据</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)  <span class="comment"># （batch，32，7，7）</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)  <span class="comment"># 展平 -1把数据编程32*7*7  (batch,32*7*7)</span></span><br><span class="line">        output = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">cnn=CNN()</span><br><span class="line">print(cnn)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>CNN(<br>  (conv1): Sequential(<br>    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br>    (1): ReLU()<br>    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>  )<br>  (conv2): Sequential(<br>    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br>    (1): ReLU()<br>    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>  )<br>  (out): Linear(in_features=1568, out_features=10, bias=True)<br>)</p>
</blockquote>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   <span class="comment"># optimize all cnn parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()   <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training and testing</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> enumerate(train_loader):   <span class="comment"># 分配 batch data, normalize x when iterate train_loader</span></span><br><span class="line">        output = cnn(b_x)               <span class="comment"># cnn output</span></span><br><span class="line">        loss = loss_func(output, b_y)   <span class="comment"># cross entropy loss</span></span><br><span class="line">        optimizer.zero_grad()           <span class="comment"># clear gradients for this training step</span></span><br><span class="line">        loss.backward()                 <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">        optimizer.step()                <span class="comment"># apply gradients</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> step%<span class="number">50</span>==<span class="number">0</span>: </span><br><span class="line">            test_output=cnn(test_x)</span><br><span class="line">            pred_y=torch.max(test_output,<span class="number">1</span>)[<span class="number">1</span>].data.squeeze()</span><br><span class="line">            accuracy=float(sum(pred_y==test_y))/test_y.size(<span class="number">0</span>)</span><br><span class="line">            print(<span class="string">'Epoch:'</span>,epoch,<span class="string">'| train loss %.4f'</span> % loss.data,<span class="string">'| test accuracy: %.2f'</span>% accuracy)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Epoch: 0 | train loss 2.2979 | test accuracy: 0.13<br>Epoch: 0 | train loss 0.2706 | test accuracy: 0.84<br>Epoch: 0 | train loss 0.2919 | test accuracy: 0.89<br>Epoch: 0 | train loss 0.1879 | test accuracy: 0.92</p>
<p>……</p>
</blockquote>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_output = cnn(test_x[:<span class="number">10</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.max用于分类问题，选取最大的概率</span></span><br><span class="line"><span class="string">torch.max(input,dim)</span></span><br><span class="line"><span class="string">input是softmax函数输出的一个tensor</span></span><br><span class="line"><span class="string">dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值</span></span><br><span class="line"><span class="string">函数会返回两个tensor，第一个tensor是每行的最大值；第二个tensor是每行最大值的索引。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line">print(pred_y, <span class="string">'prediction number'</span>)</span><br><span class="line">print(test_y[:<span class="number">10</span>].numpy(), <span class="string">'real number'</span>)</span><br></pre></td></tr></table></figure>

<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>数据间的顺序关联</p>
<h2 id="分类-1"><a href="#分类-1" class="headerlink" title="分类"></a>分类</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dsets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)  <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span>  <span class="comment"># 训练整批数据多少次, 为了节约时间, 我们只训练一次</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">TIME_STEP = <span class="number">28</span>  <span class="comment"># rnn 时间步数 / 图片高度 每28步中的一步读取一行信息</span></span><br><span class="line">INPUT_SIZE = <span class="number">28</span>  <span class="comment"># rnn 每步输入值 / 图片每行像素 一行信息包括28个像素点</span></span><br><span class="line">LR = <span class="number">0.01</span>  <span class="comment"># learning rate</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">True</span>  <span class="comment"># 如果你已经下载好了mnist数据就写上 Fasle</span></span><br><span class="line"></span><br><span class="line">train_data = dsets.MNIST(root=<span class="string">'./mnist'</span>, train=<span class="literal">True</span>, transform=transforms.ToTensor(), download=DOWNLOAD_MNIST)</span><br><span class="line">test_data = dsets.MNIST(root=<span class="string">'./mnist'</span>, train=<span class="literal">False</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_x = test_data.data.type(torch.FloatTensor)[:<span class="number">2000</span>] / <span class="number">255</span></span><br><span class="line">test_y = test_data.targets[:<span class="number">2000</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.LSTM(  <span class="comment"># 使用rnn准确度不高</span></span><br><span class="line">            input_size=INPUT_SIZE,  <span class="comment"># 图片每层的像素点</span></span><br><span class="line">            hidden_size=<span class="number">64</span>,</span><br><span class="line">            batch_first=<span class="literal">True</span>,  <span class="comment"># batch_size是否在第一维度</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x shape (batch, time_step, input_size)</span></span><br><span class="line">        <span class="comment"># r_out shape (batch, time_step, output_size)</span></span><br><span class="line">        <span class="comment"># h_n shape (n_layers, batch, hidden_size)   LSTM 有两个 hidden states, h_n 是分线, h_c 是主线</span></span><br><span class="line">        <span class="comment"># h_c shape (n_layers, batch, hidden_size)</span></span><br><span class="line">        <span class="comment"># None 是否有第一个hidden state</span></span><br><span class="line">        r_out, (h_n, h_c) = self.rnn(x, <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 28个output</span></span><br><span class="line">        <span class="comment"># 选最后一个output(读完所有数据)</span></span><br><span class="line">        out = self.out(r_out[:, <span class="number">-1</span>, :])  <span class="comment"># (batch,time step,input)</span></span><br><span class="line">        <span class="comment"># print(r_out)</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">		<span class="comment"># 这里还有点不明白，需要补充别的视频看一下</span></span><br><span class="line"></span><br><span class="line">rnn = RNN()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)  <span class="comment"># optimize all parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()  <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (x, b_y) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        b_x = x.view(<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>)  <span class="comment"># reshape x to (batch,time_step,input_size)</span></span><br><span class="line">        output = rnn(b_x)  <span class="comment"># rnn output</span></span><br><span class="line">        loss = loss_func(output, b_y)  <span class="comment"># cross entropy loss</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># clear gradients for this training step</span></span><br><span class="line">        loss.backward()  <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">        optimizer.step()  <span class="comment"># apply gradients</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每50步</span></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            test_output = rnn(test_x)</span><br><span class="line">            <span class="comment">#torch.max(input,dim) input是softmax函数输出的一个tensor</span></span><br><span class="line">            <span class="comment"># dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值</span></span><br><span class="line">            <span class="comment"># 函数会返回两个tensor，第一个tensor是每行的最大值；第二个tensor是每行最大值的索引</span></span><br><span class="line">            pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.squeeze()</span><br><span class="line">            <span class="comment"># torch.size()返回（行，列）</span></span><br><span class="line">            accuracy = float(sum(pred_y == test_y)) / test_y.size(<span class="number">0</span>)</span><br><span class="line">            print(<span class="string">'Epoch:'</span>, epoch, <span class="string">'| train loss %.4f'</span> % loss.data, <span class="string">'| test accuracy: %.2f'</span> % accuracy)</span><br><span class="line"></span><br><span class="line">test_output = rnn(test_x[:<span class="number">10</span>].view(<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line">print(pred_y, <span class="string">'prediction number'</span>)</span><br><span class="line">print(test_y[:<span class="number">10</span>], <span class="string">'real number'</span>)</span><br></pre></td></tr></table></figure>


<blockquote>
<p>RNN(<br>      (rnn): LSTM(28, 64, batch_first=True)<br>      (out): Linear(in_features=64, out_features=10, bias=True)<br>)</p>
</blockquote>
<blockquote>
<p>Epoch: 0 | train loss 2.2883 | test accuracy: 0.10<br>Epoch: 0 | train loss 0.8795 | test accuracy: 0.57<br>Epoch: 0 | train loss 1.0830 | test accuracy: 0.76</p>
</blockquote>
<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>用sin预测cos</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.manual_seed(1)    # reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">TIME_STEP = <span class="number">10</span>  <span class="comment"># rnn time step</span></span><br><span class="line">INPUT_SIZE = <span class="number">1</span>  <span class="comment"># rnn input size</span></span><br><span class="line">LR = <span class="number">0.02</span>  <span class="comment"># learning rate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># show data</span></span><br><span class="line">steps = np.linspace(<span class="number">0</span>, np.pi * <span class="number">2</span>, <span class="number">100</span>, dtype=np.float32)  <span class="comment"># float32 for converting torch FloatTensor</span></span><br><span class="line">x_np = np.sin(steps)</span><br><span class="line">y_np = np.cos(steps)</span><br><span class="line">plt.plot(steps, y_np, <span class="string">'r-'</span>, label=<span class="string">'target (cos)'</span>)</span><br><span class="line">plt.plot(steps, x_np, <span class="string">'b-'</span>, label=<span class="string">'input (sin)'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.RNN(</span><br><span class="line">            input_size=INPUT_SIZE,<span class="comment"># 1 用sin的数据预测cos</span></span><br><span class="line">            hidden_size=<span class="number">32</span>,  <span class="comment"># rnn hidden unit（time step）</span></span><br><span class="line">            num_layers=<span class="number">1</span>,  <span class="comment"># number of rnn layer</span></span><br><span class="line">            batch_first=<span class="literal">True</span>,  <span class="comment"># input &amp; output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)</span></span><br><span class="line">        )</span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, h_state)</span>:</span></span><br><span class="line">        <span class="comment"># x (batch, time_step, input_size)</span></span><br><span class="line">        <span class="comment"># h_state (n_layers, batch, hidden_size)</span></span><br><span class="line">        <span class="comment"># r_out (batch, time_step, hidden_size)</span></span><br><span class="line">        r_out, h_state = self.rnn(x, h_state) <span class="comment"># 将状态当作下一步的输入</span></span><br><span class="line">        <span class="comment"># r_outb包含所有的out但是h——state只包含最后一个</span></span><br><span class="line"></span><br><span class="line">        outs = []  <span class="comment"># save all predictions</span></span><br><span class="line">        <span class="keyword">for</span> time_step <span class="keyword">in</span> range(r_out.size(<span class="number">1</span>)):  <span class="comment"># calculate output for each time step</span></span><br><span class="line">            outs.append(self.out(r_out[:, time_step, :]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将list变成tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.stack(outs, dim=<span class="number">1</span>), h_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># instead, for simplicity, you can replace above codes by follows</span></span><br><span class="line">        <span class="comment"># r_out = r_out.view(-1, 32)</span></span><br><span class="line">        <span class="comment"># outs = self.out(r_out)</span></span><br><span class="line">        <span class="comment"># outs = outs.view(-1, TIME_STEP, 1)</span></span><br><span class="line">        <span class="comment"># return outs, h_state</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># or even simpler, since nn.Linear can accept inputs of any dimension</span></span><br><span class="line">        <span class="comment"># and returns outputs with same dimension except for the last</span></span><br><span class="line">        <span class="comment"># outs = self.out(r_out)</span></span><br><span class="line">        <span class="comment"># return outs</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rnn = RNN()</span><br><span class="line">print(rnn)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)  <span class="comment"># optimize all cnn parameters</span></span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">h_state = <span class="literal">None</span>  <span class="comment"># for initial hidden state</span></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.ion()  <span class="comment"># continuously plot</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    start, end = step * np.pi, (step + <span class="number">1</span>) * np.pi  <span class="comment"># time range</span></span><br><span class="line">    <span class="comment"># use sin predicts cos</span></span><br><span class="line">    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32,</span><br><span class="line">                        endpoint=<span class="literal">False</span>)  <span class="comment"># float32 for converting torch FloatTensor</span></span><br><span class="line">    x_np = np.sin(steps)</span><br><span class="line">    y_np = np.cos(steps)</span><br><span class="line"></span><br><span class="line">    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])  <span class="comment"># shape (batch, time_step, input_size)</span></span><br><span class="line">    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])</span><br><span class="line"></span><br><span class="line">    prediction, h_state = rnn(x, h_state)  <span class="comment"># rnn output</span></span><br><span class="line">    <span class="comment"># !! next step is important !!</span></span><br><span class="line">    h_state = h_state.data  <span class="comment"># repack the hidden state, break the connection from last iteration</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)  <span class="comment"># calculate loss</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># clear gradients for this training step</span></span><br><span class="line">    loss.backward()  <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">    optimizer.step()  <span class="comment"># apply gradients</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># plotting</span></span><br><span class="line">    plt.plot(steps, y_np.flatten(), <span class="string">'r-'</span>)</span><br><span class="line">    plt.plot(steps, prediction.data.numpy().flatten(), <span class="string">'b-'</span>)</span><br><span class="line">    plt.draw()</span><br><span class="line">    plt.pause(<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>




<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>解决RNN的梯度消失和梯度爆炸</p>
<p>增加输入控制、输出控制和忘记控制</p>
<p><strong>感觉RNN部分没听太明白，还需要补充其它资料再学一下</strong></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Lxm</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://gitee.com/michelle19l/michelle19l/2021/03/31/python/pytorch/morvan/">https://gitee.com/michelle19l/michelle19l/2021/03/31/python/pytorch/morvan/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="/img/timg.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2021/04/16/python/pytorch/mnist%E6%96%87%E4%BB%B6%E5%BC%95%E5%85%A5/"><img class="prev_cover lazyload" data-src="/images/pytorch.jfif" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">pytorch中引入mnist数据集</div></div></a></div><div class="next-post pull_right"><a href="/2021/02/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><img class="next_cover lazyload" data-src="/img/timg.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">机器学习/机器学习基础</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/04/16/python/pytorch/mnist文件引入/" title="pytorch中引入mnist数据集"><img class="relatedPosts_cover lazyload"data-src="/images/pytorch.jfif"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-04-16</div><div class="relatedPosts_title">pytorch中引入mnist数据集</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/30/python大作业报告/" title="python大作业报告"><img class="relatedPosts_cover lazyload"data-src="/images/timg.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-30</div><div class="relatedPosts_title">python大作业报告</div></div></a></div><div class="relatedPosts_item"><a href="/2020/05/25/list打乱/" title="python列表随机打乱顺序"><img class="relatedPosts_cover lazyload"data-src="/images/python.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-25</div><div class="relatedPosts_title">python列表随机打乱顺序</div></div></a></div><div class="relatedPosts_item"><a href="/2020/05/08/virtualenv/" title="Python virtualenv使用"><img class="relatedPosts_cover lazyload"data-src="/images/python.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-08</div><div class="relatedPosts_title">Python virtualenv使用</div></div></a></div><div class="relatedPosts_item"><a href="/2020/01/21/python/argparse/" title="利用argparse编写python命令行工具"><img class="relatedPosts_cover lazyload"data-src="/images/python.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-01-21</div><div class="relatedPosts_title">利用argparse编写python命令行工具</div></div></a></div></div><div class="clear_both"></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Lxm</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://michelle19l.gitee.io/">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script id="canvas_nest" color="87,250,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/local-search.js"></script></body></html>